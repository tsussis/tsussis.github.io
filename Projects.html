<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>Projects Page</title>
        <link rel="stylesheet" href="styles.css" />
        <script src="script.js"></script>
    </head>
    
    <body>
       <nav>
           <a href="index.html">TJS</a>
           <ul>
               <li><a href="About%20Me.html">About Me</a></li>
               <li><a href="Projects.html">Projects</a></li>
               <li><a href="Classes.html">Designs</a></li>
               <li><a href="Photos.html">Photos</a></li>
           </ul>
        </nav>
        
        <section class="Main">
      <h1>Projects</h1>
      <p>Bigger things I'm currently working on.</p>
    </section>

    <section class = project-panel>
      <ul class="logo-grid">

        <li>
          <img src="/images/project1.png" alt="Project One preview" title="Project One">
          <h2 style= "color:#ffff6a">Project One</h2>
          <p>During my Co-op for Schneider (ongoing) my group and I are working on developing an
              an improved version of the fight power app, where contracted truck drivers can
               find and select loads. I am the UI/UX lead, working on improving the 
              interface to be more intuitive and increase driver engagement and ultimately 
              freight selection quantity. The current feature we are implementing is new driver profitability tools, 
              where drivers can get a better idea of not only their RPM and Loaded RPM (rate during the time they 
              have the freight loaded) but also their hourly and daily rate. </p>
        </li>

        <li>
          <img src="/images/project2.png" alt="Project Two preview" title="Project Two">
          <h2 style= "color:#ffff6a">Project Two</h2>
            <p>
              <em>Intersect VST</em> is a plugin for a DAW (Digital Audio Workstation) that I created. 
              It is a generative harmonic sequencer where two (or more) agents ("fingers") travel along a given scale 
              or keyspace according to mathematical rules. Each agent’s motion is determined by:
            </p>
            <ol>
              <li><strong>Step size</strong> e.g. +4 semitones per beat, −1 per bar</li>
              <li><strong>Directionality</strong> up down, bounce, wrap, etc.</li>
              <li><strong>Timing grid</strong> e.g. 4/4 vs 7/8, or note division like every 3 sixteenths</li>
              <li><strong>Reset rules</strong> e.g. return to tonic after 12 notes, reverse on boundary</li>
            </ol>

            <p>
              Whenever two (or more) agents collide (land on the same key), a MIDI note is triggered.
            </p>

        </li>
<!-- Background image: Neural network abstract by @boliviainteligente via Unsplash -->
        <li>
          <img src="/images/project3.jpg" alt="Project Three preview" title="Project Three">
          <h2 style= "color:#ffff6a">Project Three</h2>
          <p>I developed a convolutional neural network (CNN) that takes input from a live webcam where the user signs ASL 
              (American Sign Language) and converts it to text on a screen that could then be read aloud by a 
              text-to-speech program. The idea is to bridge the communication gap between non-deaf people and deaf users, 
              whose primary language is ASL. Along with a few friends, we are still iterating on the idea.</p>
        </li>

      </ul>
    </section>
        
    </body>
    
</html>
